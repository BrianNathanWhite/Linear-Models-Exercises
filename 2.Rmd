---
title: "STOR 664 HW 2"
author: "Brian N. White"
date: "9/9/2020"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Problem 20

```{r}
library(data.table)
dmark <- fread('http://rls.sites.oasis.unc.edu/faculty/rs/source/Data/dmark.dat')
#pull in data from website
head(dmark)

colnames(dmark)[] <- c("week", "mark", "pound")
#give variables descriptive names
colnames(dmark)

attach(dmark)
#column names of mydat recognized independently
```

$\textbf{(a)}$

Note, there is strong visual evidence of autocorrelation in the time series.
```{r}
par(mfrow=c(2, 2))
plot(mark~week)
plot(pound~week)
plot(pound~mark)
```
$\textbf{(b)}$

Assume the individaul weekly observaions are independent. The linear regression equation $\hat y_{i}=\hat\beta_{0}+\hat\beta_{1}x_{i}$ is computed via the code below. A point estimate for $\hat\beta_{1}$ is approximately $-2.9$ with a 90% confidence interval of $[-5.12, -0.69]$. Consider the following hypotheses: $H_{0}: \beta_{1}=0$ vs $H_{1}: \beta_{1}\neq{0}$ with $\alpha=0.1$. Observe that $0$ is not an element of $[-5.12, -0.69]$, the 90% confidence interval for $\beta_{1}$. Thus, $H_{0}$ is rejected; there is evidence to suggest that $\beta_{1}\neq{0}$.
```{r}
mp_lm <- lm(pound~mark)
summary(mp_lm)
confint(mp_lm, level=0.90)

plot(pound~mark)
abline(mp_lm, col="blue")
```
$\textbf{(c)}$

Inspection of the first 10 autocorrelation coeffecients are computed from the residuals via the acf(.) command. Inspection of these, together with the heuristic $\frac{2}{\sqrt{n}}=\frac{2}{\sqrt{51}}\approx0.28$, suggests that the first 8 autocorrelations are statistically significant. There is clearly evidence that autocorrelation is present. A more precise test, such as the Durbin-Watson could be performed to confirm this heuristic argument.
```{r}
#the residuals of the linear model in question
mp_residuals <- mp_lm$residuals 
#the first 10 serial correlations are computed with
#approximate 95% error bounds if the true time series is independent.
mp_ac <- acf(mp_residuals, lag.max=10) 
mp_ac

#the heursitic used to determine statistical significance of the autocorrelations
heuristic<-2/sqrt(51) 
#the indices of the autocorrelations that are statistically significant.
which(abs(mp_ac$acf)>heuristic)


#Code to compute the Durbin Watson test statistic
dw_num <- rep(0,50)
for(i in 2:51){
  dw_num[i] <- (mp_residuals[i]-mp_residuals[i-1])^2
}

D=sum(dw_num)/sum(mp_residuals^2)
D
```
$\textbf{(d)}$
As autocorrelation is present, the standard deviation of the least squared estimates must be corrected. In particular, the corrected standard deviation of $\hat\beta_{1}$ is computed below for K=8. Note, this corrected value is about 3.77, in contrast to the original value of 1.32. With this new value, observe that the test statistic for the previously considerd hypotheses is $t=-\frac{2.195}{3.77}\approx-0.771$. The p-value associated with this test statistic, with respect to a $t_{n-2}$ distribution, is greater than the 0.1 threshold. Thus, in contrast to the previous conclusion, the evidence does not support the rejection of $H_{0}$. In otherwords, there is not statistically significant evidence to suggest that $\beta_{1}\neq{0}$.
```{r}
rse <- sum((mp_lm$residuals)^2)/49
beta1_se=sqrt(rse/sum((mark-mean(mark))^2))
beta1_se

#the denominator of r_x(k) for all k=1,..,8
k_denom <- vector()
  for(i in 1:51){
    k_denom[i] <- (mark[i]-mean(mark))^2
  }

#dummy vector to be used in the for loop below
rxk_terms <-list(vector(), vector(), vector(), vector(), vector(), vector(), vector(), vector())

#the lag
K=8

#this for loop generates the vectors that are stored in rxk_terms. Each one contains the terms of the sum in
#the numerator of the ratio that defines r_x(k)
for(i in 1:K){
  for(j in 1:(51-i)){
    rxk_terms[[i]][j] <- (mark[j]-mean(mark))*(mark[j+i]-mean(mark))
  }
}

#dummy vector to be used in the for loop below
a <- vector()

#this for loop fills the dummy vector a with the r_x(k) values where k=1,...,8; where the k'th entry corresponds to r_x(k).
for(i in 1:K){
  a[i] <-as.numeric(lapply(rxk_terms, sum)[i])/sum(k_denom)
}

#create vector of terms of the sum in the second term of 
#the scaling factor for the corrected variance slope estimate
b <- vector()
for(i in 1:8){
  b[i] <- (mp_ac$acf[i])*a[i]
}

#the slope estimate standard deviation corrected for 
#the presence of autocorrelation in the residuals.
beta1_se_corrected <- sqrt(((beta1_se)^2)*(1+2*sum(b)))
beta1_se_corrected

#t statistic with new standard error
t=(mp_lm$coefficients[2])/beta1_se_corrected
t

qt(.05, 49)
```

### Problem 21

First, the data set is imported and the columns given descriptive names.
```{r import data set}
marathon <- fread('http://rls.sites.oasis.unc.edu/faculty/rs/source/Data/marathon.dat')
#pull in data from website
head(marathon)

colnames(marathon)[] <- c("length", "count")
#give variables descriptive names
colnames(marathon)

#column names of mydat recognized independently
```
Next, approximate inverse regression is performed. Note, a zero in the length column corresponds to a missing value. I will use inverse regression to predict these unknown length values from the corresponding known count value. Further, I will produce standard errors for these estimates. The predicted values, along with their corresponding y values and standard errors, are compiled in the data frame named 'results_df'. This data frame is output below.
```{r message=FALSE}
#remove rows with unkown x values from data
marathon_known <- marathon[-c(which(marathon$length==0)),] 

#make variable names usable without reference to data set
attach(marathon_known)

#perform linear regression on the known data
lm_marathon <- lm(count~length)

#create data vectors for new unknown x and known y values
new_y <- marathon$count[which(marathon$length==0)]
new_x <- vector()

#compute inverse regression estimates of 
#the unknown length values using the corresponding known y values
for(i in 1:13){
  new_x[i] <- mean(length)+(new_y[i]-lm_marathon$coefficients[1])/lm_marathon$coefficients[2]
}

#the predicted x values
new_x 
```

```{r}
#compute the prediction standard errors of the length estimates
summary(lm_marathon)
resid_se=7.652


#compute the sum of the squared known centered length values
length_c <- vector()
for(i in 1:12){
  length_c[i] <- length[i]-mean(length)
}

d=sum(length_c^2)

#compute the standard errors of the new length estimates
se_x <- vector()
for(i in 1:13){
  se_x[i] <- (resid_se/lm_marathon$coefficients[2])*sqrt((1/12)+(((new_x[i]-mean(length))^2)/d)+1)
}

#standard errors of the predicted x values
se_x
```
```{r}
results_df <- data.frame(new_x=new_x, y=marathon$count[which(marathon$length==0)], se_new_x=se_x)
results_df
```
Next, I compute, via both approximate and exact methods, a 95% confidence interval for the total length of the predicted x values (i.e. the total length of the measurements which did not have a steel tape measurement).
```{r}

```
```{r}

```

