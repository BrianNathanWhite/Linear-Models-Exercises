---
title: "STOR 664 HW 2"
author: "Brian N. White"
date: "9/9/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Problem 20

```{r}
library(data.table)
dmark <- fread('http://rls.sites.oasis.unc.edu/faculty/rs/source/Data/dmark.dat')
#pull in data from website
head(dmark)

colnames(dmark)[] <- c("week", "mark", "pound")
#give variables descriptive names
colnames(dmark)

attach(dmark)
#column names of mydat recognized independently
```

$\textbf{(a)}$

Note, there is strong visual evidence of autocorrelation in the time series.
```{r}
par(mfrow=c(2, 2))
plot(mark~week)
plot(pound~week)
plot(pound~mark)
```
$\textbf{(b)}$

Assume the individaul weekly observaions are independent. The linear regression equation $\hat y_{i}=\hat\beta_{0}+\hat\beta_{1}x_{i}$ is computed via the code below. A point estimate for $\hat\beta_{1}$ is approximately $-2.9$ with a 90% confidence interval of $[-5.12, -0.69]$. Consider the following hypotheses: $H_{0}: \beta_{1}=0$ vs $H_{1}: \beta_{1}\neq{0}$ with $\alpha=0.1$. Observe that $0$ is not an element of $[-5.12, -0.69]$, the 90% confidence interval for $\beta_{1}$. Thus, $H_{0}$ is rejected; there is evidence to suggest that $\beta_{1}\neq{0}$.
```{r}
mp_lm <- lm(pound~mark)
summary(mp_lm)
confint(mp_lm, level=0.90)

plot(pound~mark)
abline(mp_lm, col="blue")
```
$\textbf{(c)}$

Inspection of the first 10 autocorrelation coeffecients are computed from the residuals via the acf(.) command. Inspection of these, together with the heuristic $\frac{2}{\sqrt{n}}=\frac{2}{\sqrt{51}}\approx0.28$, suggests that the first 8 autocorrelations are statistically significant. There is clearly evidence that autocorrelation is present. A more precise test, such as the Durbin-Watson could be performed to confirm this heuristic argument.
```{r}
mp_residuals <- mp_lm$residuals #the residuals of the linear model in question
mp_ac <- acf(mp_residuals, lag.max=10) #the first 10 serial correlations are computed
#with approximate 95% error bounds if the true time series is independent.
mp_ac

#the heursitic used to determine statistical significance of the autocorrelations
heuristic<-2/sqrt(51) 
#the indices of the autocorrelations that are statistically significant.
which(abs(mp_ac$acf)>heuristic)


#Code to compute the Durbin Watson test statistic
dw_num <- rep(0,50)
for(i in 2:51){
  dw_num[i] <- (mp_residuals[i]-mp_residuals[i-1])^2
}

D=sum(dw_num)/sum(mp_residuals^2)
D
```
$\textbf{(d)}$
As autocorrelation is present, the standard deviation of the least squared estimates must be corrected. In particular, the corrected standard deviation of $\hat\beta_{1}$ is computed below for K=8. Note, this corrected value is about 3.77, in contrast to the original value of 1.32. With this new value, observe that the test statistic for the previously considerd hypotheses is $t=-\frac{2.195}{3.77}\approx-0.771$. The p-value associated with this test statistic, with respect to a $t_{n-2}$ distribution, is greater than the 0.1 threshold. Thus, in contrast to the previous conclusion, the evidence does not support the rejection of $H_{0}$. In otherwords, there is not statistically significant evidence to suggest that $\beta_{1}\neq{0}$.
```{r}
summary(mp_lm)
se_slope=1.323

#confirm uncorrected standard error of slope estimate
rse <- sum((mp_lm$residuals)^2)/49
beta1_se=sqrt(rse/sum((mark-mean(mark))^2))
beta1_se

#compute corrected standard deviation of slope estimate

#the denominator of r_x(k) for all k=1,..,8
k_denom <- vector()
  for(i in 1:51){
    k_denom[i] <- (mark[i]-mean(mark))^2
  }

#compute the r_x(k) values for k=1,...,8. e.g. k1 is a vector
#containing the terms of the sum in the numerator of the ratio that defines r_x(k)
 k1 <-vector()
 for(i in 1:50){
     k1[i] <- (mark[i]-mean(mark))*(mark[i+1]-mean(mark))
 }
 
k2 <-vector()
 for(i in 1:49){
     k2[i] <- (mark[i]-mean(mark))*(mark[i+2]-mean(mark))
 }

k3 <-vector()
 for(i in 1:48){
     k3[i] <- (mark[i]-mean(mark))*(mark[i+3]-mean(mark))
 }
 
k4 <-vector()
 for(i in 1:47){
     k4[i] <- (mark[i]-mean(mark))*(mark[i+4]-mean(mark))
 }
 
k5 <-vector()
 for(i in 1:46){
     k5[i] <- (mark[i]-mean(mark))*(mark[i+5]-mean(mark))
 }

k6 <-vector()
 for(i in 1:45){
     k6[i] <- (mark[i]-mean(mark))*(mark[i+6]-mean(mark))
 }

k7 <-vector()
 for(i in 1:44){
     k7[i] <- (mark[i]-mean(mark))*(mark[i+7]-mean(mark))
 }

k8 <-vector()
 for(i in 1:43){
     k8[i] <- (mark[i]-mean(mark))*(mark[i+8]-mean(mark))
 }

#sum over the terms to compute r_x(k) for k=1,..,8
rx1 <-sum(k1)/sum(k_denom)
rx2 <-sum(k2)/sum(k_denom)
rx3 <-sum(k3)/sum(k_denom)
rx4 <-sum(k4)/sum(k_denom)
rx5 <-sum(k5)/sum(k_denom)
rx6 <-sum(k6)/sum(k_denom)
rx7 <-sum(k7)/sum(k_denom)
rx8 <-sum(k8)/sum(k_denom)

#create vector where kth entry is kth sample autocorrelation of the {mark_i} process.
a <- c(rx1, rx2, rx3, rx4, rx5, rx6, rx7, rx8)

#create vector of terms in sum in the second term of 
#the scaling factor for the corrected variance of slope estimate
b <- vector()
for(i in 1:8){
  b[i] <- (mp_ac$acf[i])*(a[i])
}

#the slope estimate standard deviation corrected for 
#the presence of autocorrelation in the residuals.
beta1_se_corrected <- sqrt(((beta1_se)^2)*(1+2*sum(b)))

beta1_se_corrected
beta1_se

t=(mp_lm$coefficients[2])/beta1_se_corrected
t
qt(.05, 49)
2*pt(t, 49)
```

### Problem 21

First, the data set is imported and the columns given descriptive names.
```{r import data set}
marathon <- fread('http://rls.sites.oasis.unc.edu/faculty/rs/source/Data/marathon.dat')
#pull in data from website
head(marathon)

colnames(marathon)[] <- c("length", "count")
#give variables descriptive names
colnames(marathon)

#column names of mydat recognized independently
```

Next, approximate inverse regression is performed. Note, a zero in the lenght column corresponds to a missing value. I will use inverse regression to predict these unknown length values from the corresponding known count value. Further, I will produce standard errors for these estimates and a 95% confidence interval for the total length of the unknown length entries.
```{r}
#return indices of the unknown x values
which(length==0)  
length(which(length==0))

#remove rows with unkown x values from data
marathon_known <- marathon[-c(which(length==0)),] 

#make variable names usable without reference to data set
attach(marathon_known)

#perform linear regression on the known data
lm_marathon <- lm(count~length)

#create data vectors for new unknown x and known y values
new_x <- vector()
new_y <- marathon[which(length==0),2]

#compute inverse regression estimates of the unknown length values using the corresponding known y values
for(i in 1:13){
  new_x[i] <- mean(length)+(new_y[i]-lm_marathon$coefficients[1])/lm_marathon$coefficients[2]
}

new_x 
```


